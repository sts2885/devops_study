{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wired-professional",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "expected-horizontal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#이건 순수하게 책에서 나온 대로만... svc까지도"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sophisticated-township",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, to_date\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf923f4b-3c41-44a1-a6fe-27b9e08b55e2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa766ac-997a-421d-81ca-ca8726c58b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#오오오 이건 생성 됐어 그리고 executor가 무한으로 생기지도 않아!!!\n",
    "session = SparkSession.builder \\\n",
    "    .config(\"spark-master\", \"k8s://https://kubernetes.default\") \\\n",
    "    .config(\"spark.kubernetes.namespace\", \"kubeflow-user-example-com\") \\\n",
    "    .config(\"spark.driver.bindAddress\", \"0.0.0.0\") \\\n",
    "    .config(\"spark.kubernetes.executor.annotation.sidecar.istio.io/inject\",\"false\") \\\n",
    "    .config(\"spark.driver.port\", \"39235\") \\\n",
    "    .config(\"spark.blockManager.port\", \"39236\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09855e3d-28a7-48d5-8a51-1a5a253058d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc9f4b1e-72ee-44cd-9553-a659ac16aac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#session은 구버전용이라 spark context를 꼭 써야 되는거 같은데... 무서운데...\n",
    "#session.parallelize([1,2,3,4,5]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c479e401-6f7b-416a-bdd4-fa8fae7bccfd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {
    "5fdc7613-42e7-4fdb-a379-09b0d7fb015c.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD1CAIAAAAzj7IHAAASfklEQVR4nO3dK3PjyN7H8dapgQvmRUgBKYOZWtYCB0shQUPDWtAiwwYuC5GgxUIHhWyLboDFtiYgNWBa7+GQ4TpAlq2bL4mdthN/P+RZ69L6a+rR77S67bRTVZUAAIv+c+wCAJydD/X/+ffz59+Pj8ctBcA5+OPTJ6d+z3pwnP/ywgXg9T04Du9ZAGwjdwDYRu4AsI3cAWAbuQPANnIHgG0nnDt55Dh+Wh67DACH9pzcKcs88n1nwY+OnQnW6slT33GinAgEDmPn3ClT3/PCrCiaDUUW3xwxeU6tHgA72zV38tu4EFIlxlQ1Y7SavGppp1NPMJ1X1SxwX6l54Nx82O2w8teTEOrbbLp89lw3mM2C1yrrrdUD4BkOM65c1gMgoswjf91wy6bBmMUQcnN6lK+9yDNGmrvFdEZnyjxtaulUsmZ7mfr96/bvJu/fTpSvvQpw9uq3lH+a/1hLKyGETLQZ22kSKYRSqt+40p0jumRi2o1LpZRqN69V65j6/N4pa+sZvd6yGK3Gd6zbXplEti9daTVovH2vlVZj/xrtFoDz9c8zcqeqTKKkEEJKNXjYm6d8lQOmfjZbuaGUXg3GJKrzIGo18lyucqduvn/ZrfX0q1lcQKt2Y0arRFebtvdzp46n1lWNrm+/H2zL65vFKQQP8NzcqaqqMrp+2IVU7YfIJHL4WNWdIF2NGnmU+4cucsd082CXejo9pda2+gpjezdt7xU7Wmt368gh42cBZ+ifZ4/vuMF0Nq+MTiZPsed3x2EmF90JH/diIsTTr9UoSZmnURRFvu87jhcXokteeiMX/H7jhZnSZs1s0vp6ithz2sKsKSaYaSVibzAss257Vz2ifd0fwQ6ue69VvdvxLkdezYAz9aJxZTeYzuZaFdlfu46WlqnveF4YZ1mWFUU/cjaYTKQQ2b3ZeJ3n1hMssyr0HH/1hcB121+in8IAll4+n+VdSlH8NKsNra6NEEKI/D5rHr/62zaJNsYYsxqx2cHl17lJZBZ6W1OgV8/YG9N8Ne2+6ClVJhFZeNvqJ63bvtx/MREiu+/vye+zNT02AH075k6ZRlGal613pvQ2LrpPWhHfLMOhzFM/zJoXkvLXkxDyy1Xguq7rumUe3Qxes9Zzp4vo8dsz25vqCa5Vp5r6JxXNxzyNWmcK0STmuu09wbUSIgtb3aHFzcovV3RxgJ0sR3o2jgSN9086E81Cyv4x3f0tMtGDceVBB6W7sTs/tqWe0f3Nuf3p8s4819gJI/Pow2v39/fGkDcOsgPnZPdxZXd6pxO1ChYpVaJN9/vBk2/zxdS2EKK7353e6UQud+m76bPfSILZXCtRxHWvZ1s97nRudHe/vlt0R4Kvq+/fSKm0Wbx+rds+Vku1utNtBwPoO8x6EmXqe/FEV/xQAcAWrCcB4AjIHQC2kTsAbGO9UABWMb4D4AjIHQC2kTsAbCN3ANhG7gCwjdwBYNuO60ls8e/nz78fHw/SFIAT9MenT3/++HGo1g6TO78fH/n6D/COPTjOAVvjPQuAbeQOANvIHQC2kTsAbCN3ANj2fnNnseb6scsAMGAxd8oyj3y/WUfPj04jE8o8bVW1ec0+AAdhK3fK1Pe8MFst2Vdk8c2xk6dMfccL41ZVosji0WWz9riEf7he12FbA47FUu7UC/epxDSLvRij1cTOtdcoU9+ri9LLqipjdKIuD3gV8/M5q6PabQ04lsN8X3mbelHxb7PVWi+uG8yOufpEmd7EhZBJb/0Z1w2mLIoBvLKTGFcuU99xolyUeeSvG/7ZNDi0GEJuTo9G35PK1HeWI835bVwIpbcvetW/bN6/bpSLMk+bY9qF5ZHjOGEmRBF7Tq+u7p0uG82jfvnNP83G1oC3ZrmC354LAG45QishhEz0cNHyqllMUw1W4dy4/mdrgU6thJBKKdVuvrPcaH1+9+P21TtX6/iNFlWvCzooe+26pM2ZIzezbFQr0V9ntf60tjXAhj0joteUrdypmhU2pVSD8GkexFUume6yxJVJlFoNw5hECdFb5niwzvEqd+rm25cdWUh4qH7SW+UaXdfZDYlW2WZxSm/J4m5lTQR2b3VszeZhOo6u5wxY8FZzp6qqyujF8r5StZ+eenn13vO0sU9iBsurDw5dPKNmsdP0923LndFGu1tHDulvGiTFWHT0yqk/mpEiyR0cy2Fzx+74jhtMZ/PK6GTyFHt+d4RictEda3EvJkI8/VoOl5RlnkZRFPm+7zhe3J/XkZdjS65/v/HCTGkzCwYDOa2mx9RD4df9Mebguve+07uudznyatbXDNE0wqxTTjDTKgu9MFOaMW68S8cYV3aD6WyuVZH9tetXUcrUdzwvjLMsy4rnTCVPJlKI7N70ruNdSlF8//sQX4TpxyWArY42n+VdSlH8NKsN/f5Hfp81T3X97Z9EG2OMWY3Y7ODy69wkMgu9qDMR5V59kaLY+MVF92IiRHbfnzTK77M1XatnGHtVak2t5VGYyUQnMguZtMK7ZCd3yjSK0rxsvTOlt3HRfYCL+GYZDmWe+mHWvOeUv56EkF+uAtd1Xdct8+hm8Jq1njtdRE/7i77u9C6Roog9x49addW/5UhzIRZvVFnorxJrUZX8cvW8Lk7xs9XfCq5V51bra7Y+1qlzNw2m35QYSZ5Oa8DbdJBBo22nj/dPutPFUvaP6e5vkYkejCsP+hDdjb35sda2DXUN5q7FcLKqN/LbHw1vl77YPPaPsWy1X7XY1hpgxduczzI6UatgkVK1v8rTPKyLqXYhRHd/PYW93KXNyHzWltyplvPi3Ulto1U776RsTdfXpSnZ2dm/xJbcabfQ+jqS7v5jNNcc3sgweYatAa/vsLnjVFUlhHhwnH3+MPuep5ep78UTXTF7A5yoPZ/xXlMn8TsJAGeF3AFgG7kDwDY7fwdjC3c6r6bHLgKALfR3ANhG7gCwjdwBYBu5A8A2cgeAbeQOANtOYh5dCPHv58+/Hx+PXQXwnv3x6dOfP34cuwohTid3fj8+HurXHwBGPTjOsUtY4D0LgG3kDgDbyB0AtpE7AGwjdwDYRu4AsI3cAWAbuQPAtreTO2XqO874OnZ51NmT10cebpWpMk8j318uK+xH6QEbB87P28mdIylT3/HCOGutjlxkcXh7wIU8y9T3/V2XbLbcGvAa3mPuBNN5Vc2CA6xbXqa+Fxf1al7LVa2M0Ym63L/xJfPzOUu+220NeA2n8vusU1SmN3EhZGJaa5cLIVw3mLLSF7CH99jfKVPfWbxp5JEzGBTqDRSVeeSPDtzkt3EhlO6Gzuj18tboT3/wZzH2VOZpc4wfpWV7rxNmQhSx5zjdWtcUNryl5Q1tag04JQdZhHT/NUx3WOl4sALwUm+94PYixv11fntbRtYq7+zavhjw6CLrrdO0EkKp/kLrnfLWXn1No917atW5tjWgqvZ7Tt/m+uj7tzDyJK55xDqLp/eDp/25blIul2I3WsllJowsfj60WHN9tW660XWZ3ZBoXcQMl2kfroq+sbDOGcN0HF0sHqjInZe08MLc6QVP+6kce0JXabNL7gx7U4OtI4eMRWGnjs2FtT6akSLJHaxzOrnzxsZ31r9nrRVcKyGy+3qkI7/PhPxytRqxaUZCGmEmxNOvZixl9V+jyl9PQqjr/hhzcN2rR1567Y/e5cYE3aUwEcy0ykIvzJRmjBtvzhvLnZcIviZyETyD2NnAu5Si+P73Ib4IM7k4wJw+8H6cQe4I9+qLFNl9XqZ/DWJn7I2knsFyr75IUcQ3G76C515MVl2plfw+63dxnm99YfU1ojCTiU5kFjJphTfnHHJnETx/3XwvhPq2enaDayWK+Kb1i4qyzKPlR3d6l0hRxJ7jR3m5Sp+yzCM/zZsWRBb6qybKPPXD3TtVS8VPs7rClsKa1LmbBtNvSowkT6c14PQcZNDodOfR2yeLwfljY9XdM0enycVwvmptEyMjv/07aVfRmWlb02p35Lg/Sj3SGlBVFePK1rlXX6QYjgG707nRSi4fVCmVvuv0VILZ3HQOqQ8yX4PlAZVJVKcFs/2rhr0q7pJevG0orOnrNJcIZrrT5xlpDTgxTlVVQogHx9lnGZk9Tz9ICwA22+cpO+AT+uA4Z9LfAXBCyB0AtpE7AGwjdwDYRu4AsI3cAWAbuQPANnIHgG3kDgDbyB0Atr3t9ST+/fz59+PjsasAXt0fnz79+ePHsas4mLedO78fH/lVF87Bg+Mcu4RD4j0LgG3kDgDbyB0AtpE7AGwjdwDYRu4AsI3cAWAbuQPANnIHgG3kDgDbyB0AtpE7AGwjdwDYRu4AsI3cAWAbuQPANnIHgG3kDgDbyB0AtpE7AGwjdwDYRu4AsI3cAWAbuQPANnIHgG1nkztlmUe+7yz4UVoOD8nT1iGO70dpPjiqTH3HHzl5bStjjQBnrqqqqqr+af7jZfY8/cUt7HqWSeTgxmVithxRH6a06TfVPXV7K0q/4N7W34pce/0jt4bXs/8jtmcjBylg2dRZ9Hfy27gQUiWmecCM0Wqy2l+mvhcXdcasnkGjEyVFkYVelO9ylUUrUiWtVozRibo84L2Yn0Vxqq0BuzpImJ12f8ckcmOfQyshhOj1a9rntjs46/o7gwNfiVaHvMphW8Prob/z3uT3mRBKzwJ3ZKc7/aaEKL7/vWWMJr+NC6H0fDrWSFt/mKkz+JNHjhPloszT5pj2QFQeOY4TZkIUsec4juO0+mFlHvkjjeZR97B6fMqJ8s2tAa/sIGF22v2dRY9GJqM9Gq3E5hGY7gHj/Z1tXaplUyPjP63TtBJCKdU7oLmc7u9ozhwZVlo2Oqx+8WltazhF76y/cx65U1UmUVIIIfvDxDu8aphEto4Yzx2tdnhu6ye9VYHRdWB0Q6KVkGZxSut6w3KbN7ymWaNVu+DWGcN05D3rrXhnuXMu71nudDY3OpmILPQ6Ly/lr6ddzp9cbHuB2mrxPjdfvdC5wXSulRDZffsVR+n5dHGIG8y0EqL4aTY0exsXMjHLc4QbzL6p1jnBTKsivs3L/DYulJ4F+94IsK9zyR0hhHCD6WxeGZ1MnmLPXwxnuBeTLQ+2+VkIeeltbf7p18YxoPLXkxDquv/UB9e9953epbzLNRP8bc0QTSPMOuUEM62y0AszUgen4Zxyp+YG09lcqyL7q+70eJey3+PoyO+z7d0d71LuMPi8kwP0rIATd365I4RY5ETdyalnrLIwGv1acR6FmZDJ1y3dBPfqixRFfLPhi8zuxWQs3/L7bKfe1EZjQzStqbU8CjOZ6ERmIZNWOAXnkDtlGkVpXq6GdMr0Nm69OwVfEylEFnp+1DkqTyM/zHaaHRfu9C6Roog9p9NI/fOMNBdi8UaVhf4q4Mo89cNMyC9Xz+viFD/N6grBtRJFfNOKzbLMo9bHOnXupsH0mxIjydNpDbDiIIPVpz2fNf7rhe700+gMd32Y2d7Yoq11jfTnqzr6k1W9abH+HFT7+ovNG38F0p2x6n9pYKQ1nKR3Np91Drmz+MnD8gmTUo19lccY3TpISNl86ETPptwZNlJP3HdnvZXs7GyXsEPutFtoXVZ376+55nCefJg8w9Zwet5Z7jhVVQkhHhznv1U19r/VO9nz9Be3sP91N2t+uCWUNuPfZwasOMj/q+/TyAGftQfHOYfxnZdzp/P6q3tZ6G384xcAnuHDsQs4dW4wq6rZsasA3hX6OwBsI3cA2EbuALCN3AFgG7kDwDZyB4Bt5A4A28gdALaROwBsI3cA2EbuALCN3AFgG7kDwDZyB4Bt5A4A28gdALaROwBsI3cA2EbuALCN3AFgG7kDwDZyB4Bt5A4A28gdALaROwBsI3cA2EbuALCN3AFgG7kDwDZyB4BtH45dwF4+fPz44DjHrgJ4dR8+fjx2CYf0tnPH/9//jl0CgGfjPQuAbeQOANvIHQC2kTsAbCN3ANhG7gCwjdwBYBu5A8A2cgeAbeQOANvIHQC2kTsAbCN3ANhG7gCwjdwBYBu5A8A2cgeAbeQOANvIHQC2kTsAbCN3ANh2KutJsCIN8NpOZzGcU8kdVqQBzgfvWQBsI3cA2EbuALCN3AFgG7kDwDZyB4Bt5A4A28gdALaROwBsI3cA2EbuALCN3AFgG7kDwDZyB4Bt5A4A28gdALaROwBsI3cA2EbuALDtMH9fmb/KDrxvh/2b8IfJHf4qO4Dd8Z4FwDZyB4Bt5A4A28gdALaROwBsI3cA2EbuALCN3AFgG7kDwDZyB4Bt5A4A28gdALaROwBsI3cA2EbuALCN3AFgG7kDwDZyB4Bt5A4A28gdALYt/q47C0IAsOPDx4//B7jC62VoMoWqAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "id": "fcfba60f-bcd1-4f23-8faa-f6b92d4252e3",
   "metadata": {},
   "source": [
    "# 스파크 세션이란?\n",
    "https://velog.io/@6v6/SparkSession-SparkContext-%EC%B0%A8%EC%9D%B4\n",
    "\n",
    "![image.png](attachment:5fdc7613-42e7-4fdb-a379-09b0d7fb015c.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0660f8d9-f23f-482b-b633-b480ccbb259c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53c1aeac-be5d-459e-ad70-13d2bb5b529b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f578a024580>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pyspark\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark import SparkContext, SparkConf\n",
    "conf = SparkConf().setAppName('sparktest')\n",
    "conf.set(\"spark-master\", \"k8s://https://kubernetes.default\")\n",
    "conf.set(\"spark.kubernetes.namespace\", \"kubeflow-user-example-com\")\n",
    "conf.set(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "conf.set(\"spark.kubernetes.executor.annotation.sidecar.istio.io/inject\",\"false\")\n",
    "conf.set(\"spark.driver.host\", \"jupyter-2\")\n",
    "conf.set(\"spark.driver.port\", \"39235\")\n",
    "conf.set(\"spark.blockManager.port\", \"39236\")\n",
    "conf.set(\"spark.kubernetes.executor.instances\", \"3\")\n",
    "#그러고 보니 위에서는 service account name을 안적었는데 괜찮은건가?.... 오히려 my-release-spark랑 둘이 충돌나나?\n",
    "#컨테이너 이미지도 안적었는데?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f6b3981c-41ea-4c7c-b6fe-7d78b158c85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea34ab50-509f-443e-8ea4-773574b60de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.parallelize([1,2,3,4,5]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5948b90f-3534-433f-8265-86277124ba89",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d526987-2db2-4280-b1a6-a07f68126c5d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d127f6-d23e-4e73-b6ea-1970874f5f9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b74de71-67b2-46f8-8729-c3ea42e9d616",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cb3fda-0b05-45bd-83be-3dae23164391",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latin-afternoon",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-vintage",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/@szinck/setting-up-pyspark-jupyter-and-minio-on-kubeflow-kubernetes-aab98874794f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "other-combine",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "entertaining-jefferson",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import findspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "everyday-speaker",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "prescription-humidity",
   "metadata": {},
   "outputs": [],
   "source": [
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "coated-belly",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proof-saying",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fabulous-blood",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f4b384b8a30>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "#.svc는 있나 없나 결과가 똑같이 실패네 excutor 계속 생산\n",
    "conf = SparkConf().setAppName('sparkTest').setMaster(\"k8s://https://kubernetes.default.svc:443\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.namespace\", \"kubeflow-user-example-com\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.authenticate.driver.serviceAccountName\", \"my-release-spark\")#\"default-editor\")\n",
    "\n",
    "#mountable secret이라는게 없어서 이건 넣고 싶어도 못넣음\n",
    "#conf.set(\"spark.kubernetes.authenticate.driver.oauthToken\", \"ABCXYZ...\")\n",
    "\n",
    "#이거...는... 내가 spark만 넣은 컨테이너 하나 둬야 되나?\n",
    "conf.set(\"spark.kubernetes.container.image\", \"gcr.io/spark-operator/spark-py:v3.1.1\") \n",
    "\n",
    "conf.set(\"spark.hadoop.fs.s3a.access.key\", \"AKIATHOTITFUG3RFBT7L\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.secret.key\", \"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\")\n",
    "conf.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\")\n",
    "\n",
    "#추가해쓴ㄴ데 역시 또...\n",
    "conf.set(\"spark.kubernetes.driver.annotation.sidecar.istio.io/inject\", \"false\")\n",
    "conf.set(\"spark.kubernetes.executor.annotation.sidecar.istio.io/inject\", \"false\")\n",
    "\n",
    "conf.set(\"spark.kubernetes.allocation.batch.size\", \"5\")\n",
    "conf.set(\"spark.kubernetes.executor.instances\", \"1\")\n",
    "conf.set(\"spark.driver.bindAddress\", \"0.0.0.0\")\n",
    "conf.set(\"spark.driver.host\", \"jupyter\")\n",
    "conf.set(\"spark.driver.port\", \"37371\")\n",
    "conf.set(\"spark.blockManager.port\", \"6060\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fewer-courage",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "utility-boring",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "automatic-denial",
   "metadata": {},
   "outputs": [],
   "source": [
    "executor 또 새로 처음부터 만들어지네"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charged-basics",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "attempted-committee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-sport",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "failing-azerbaijan",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "moved-crisis",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"s3://s3-kubernetes-bucket-sts/kube_join_url.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "moral-donna",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_rdd = sc.textFile(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "royal-maine",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:275)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eadc8ce8ef35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforeach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprintln\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1812\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1813\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1814\u001b[0;31m             \u001b[0msock_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1815\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msock_info\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1816\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:275)\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:244)\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:332)\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:208)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:291)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:287)\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2328)\n\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1019)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:405)\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1018)\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:193)\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "my_rdd.collect().foreach(println)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "female-shopper",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-patrol",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "informative-corps",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f6aa81c5c70>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "conf2 = pyspark.SparkConf().setAppName('appName').setMaster(\"local\")\n",
    "\n",
    "conf2.set(\"spark.hadoop.fs.s3a.access.key\", \"AKIATHOTITFUG3RFBT7L\")\n",
    "conf2.set(\"spark.hadoop.fs.s3a.secret.key\", \"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\")\n",
    "conf2.set(\"spark.hadoop.fs.s3a.endpoint\", \"s3.us-east-1.amazonaws.com\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "steady-proxy",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "qualified-preliminary",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.config(conf=conf2) \\\n",
    "                            .appName(\"Learning_Spark\") \\\n",
    "                            .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setSystemProperty(\"com.amazonaws.services.s3.enableV4\", \"true\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-browser",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "professional-alberta",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o74.text.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:646)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-528e9326700b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#이건 sc 말고 spark 인 상태에서 해야 함\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmy_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     def csv(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o74.text.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:646)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "#이건 sc 말고 spark 인 상태에서 해야 함\n",
    "my_df = spark.read.text(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "obvious-messaging",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RDD' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-1d689ff79045>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmy_rdd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RDD' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "my_rdd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sapphire-stereo",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "silver-sacramento",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coordinate-trustee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "forty-turkish",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o33.text.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:646)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-d300b4728349>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtest\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"s3://s3-kubernetes-bucket-sts/kube_join_url.txt\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mtext\u001b[0;34m(self, paths, wholetext, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter)\u001b[0m\n\u001b[1;32m    600\u001b[0m             \u001b[0mpaths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     def csv(\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1320\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m         return_value = get_return_value(\n\u001b[0m\u001b[1;32m   1323\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[1;32m   1324\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/errors/exceptions/captured.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdeco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 169\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m             \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconvert_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjava_exception\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.8/site-packages/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mOUTPUT_CONVERTER\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway_client\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mREFERENCE_TYPE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m                 raise Py4JJavaError(\n\u001b[0m\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m                     format(target_id, \".\", name), value)\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o33.text.\n: org.apache.hadoop.fs.UnsupportedFileSystemException: No FileSystem for scheme \"s3\"\n\tat org.apache.hadoop.fs.FileSystem.getFileSystemClass(FileSystem.java:3443)\n\tat org.apache.hadoop.fs.FileSystem.createFileSystem(FileSystem.java:3466)\n\tat org.apache.hadoop.fs.FileSystem.access$300(FileSystem.java:174)\n\tat org.apache.hadoop.fs.FileSystem$Cache.getInternal(FileSystem.java:3574)\n\tat org.apache.hadoop.fs.FileSystem$Cache.get(FileSystem.java:3521)\n\tat org.apache.hadoop.fs.FileSystem.get(FileSystem.java:540)\n\tat org.apache.hadoop.fs.Path.getFileSystem(Path.java:365)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.$anonfun$checkAndGlobPathIfNecessary$1(DataSource.scala:724)\n\tat scala.collection.immutable.List.map(List.scala:293)\n\tat org.apache.spark.sql.execution.datasources.DataSource$.checkAndGlobPathIfNecessary(DataSource.scala:722)\n\tat org.apache.spark.sql.execution.datasources.DataSource.checkAndGlobPathIfNecessary(DataSource.scala:551)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:404)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:229)\n\tat org.apache.spark.sql.DataFrameReader.$anonfun$load$2(DataFrameReader.scala:211)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:211)\n\tat org.apache.spark.sql.DataFrameReader.text(DataFrameReader.scala:646)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.lang.Thread.run(Thread.java:750)\n"
     ]
    }
   ],
   "source": [
    "test = spark.read.text(\"s3://s3-kubernetes-bucket-sts/kube_join_url.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "active-haiti",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eleven-sensitivity",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "test = spark.read.text(\"s3://s3-kubernetes-bucket-sts/kube_join_url.txt\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "sc = pyspark.SparkContext(conf=conf)\n",
    "#끝나면\n",
    "#sc.stop()\n",
    "\n",
    "spark = SparkSession(sc)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parental-spirituality",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "graphic-external",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-crime",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "solar-sheep",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "chemical-internship",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fleet-pottery",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "distinguished-burton",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "purple-factory",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-lighting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#블로그 보면 driver가 default-editor라는 sa를 통해서 kubernetes api에 authenticate를 한다고 함. (kubeflow 네임스페이스에 있다고 했는데)\n",
    "#근데 나는 설치를 kubeflow-user-example-com에 해둬서 여기에 있음.\n",
    "#그리고 kubeflow를 통해 설치한 namespace => 얘는 anonymous라고 했다고 함 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-enterprise",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "settled-somewhere",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().setAppName('sparktest').setMaster('k8s://https://kubernetes.default.svc:443')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "radio-piece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f4b087467d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.kubernetes.namespace\", \"kubeflow-user-example-com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "residential-forth",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.4.1\n"
     ]
    }
   ],
   "source": [
    "print(pyspark.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "second-revelation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f4b087467d0>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.kubernetes.container.image\",\n",
    "         \"docker.io/stevenzinck/spark:2.4.4-hadop_3.2.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "musical-paris",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f4b087467d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.kubernetes.allocation.batch.size\" , \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fresh-auditor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f4b087467d0>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.kubernetes.executor.instances\", \"1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "vocal-intersection",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f4b087467d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.driver.bindAddress\", \"0.0.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "twelve-doctor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f4b087467d0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.driver.host\", \"jupyter\") #jupyter가 spark driver 이름임 => service를 통해서 생성한"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "spoken-nancy",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f4b087467d0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.driver.port\", \"37371\") #service에서 지정한 포트임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ideal-routine",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pyspark.conf.SparkConf at 0x7f4b087467d0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conf.set(\"spark.blockManager.port\", \"6060\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "celtic-pride",
   "metadata": {},
   "source": [
    "kubectl describe sa default-editor -n kubeflow-user-example-com  \n",
    "Name:                default-editor  \n",
    "Namespace:           kubeflow-user-example-com  \n",
    "Labels:              <none>  \n",
    "Annotations:         <none>  \n",
    "Image pull secrets:  <none>  \n",
    "Mountable secrets:   <none>  \n",
    "Tokens:              <none>  \n",
    "Events:              <none>  \n",
    "\n",
    "    tocken None인데?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "powerful-poverty",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unable-piece",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pod가 dynamic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rough-rough",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "federal-isaac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alive-cutting",
   "metadata": {},
   "outputs": [],
   "source": [
    "#이 주피터에 spark, java 가 깔려 있어야 되나?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "clinical-chicken",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "JAVA_HOME is not set\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Java gateway process exited before sending its port number",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m sc \u001b[38;5;241m=\u001b[39m \u001b[43mSparkContext\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/context.py:198\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m gateway \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m gateway\u001b[38;5;241m.\u001b[39mgateway_parameters\u001b[38;5;241m.\u001b[39mauth_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    194\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    195\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m is not allowed as it is a security risk.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 198\u001b[0m \u001b[43mSparkContext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ensure_initialized\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgateway\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgateway\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_do_init(\n\u001b[1;32m    201\u001b[0m         master,\n\u001b[1;32m    202\u001b[0m         appName,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m         memory_profiler_cls,\n\u001b[1;32m    213\u001b[0m     )\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/context.py:432\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    430\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_lock:\n\u001b[1;32m    431\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m SparkContext\u001b[38;5;241m.\u001b[39m_gateway:\n\u001b[0;32m--> 432\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_gateway \u001b[38;5;241m=\u001b[39m gateway \u001b[38;5;129;01mor\u001b[39;00m \u001b[43mlaunch_gateway\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    433\u001b[0m         SparkContext\u001b[38;5;241m.\u001b[39m_jvm \u001b[38;5;241m=\u001b[39m SparkContext\u001b[38;5;241m.\u001b[39m_gateway\u001b[38;5;241m.\u001b[39mjvm\n\u001b[1;32m    435\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m instance:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/pyspark/java_gateway.py:106\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[0;34m(conf, popen_kwargs)\u001b[0m\n\u001b[1;32m    103\u001b[0m     time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39misfile(conn_info_file):\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJava gateway process exited before sending its port number\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    108\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(conn_info_file, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m info:\n\u001b[1;32m    109\u001b[0m     gateway_port \u001b[38;5;241m=\u001b[39m read_int(info)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Java gateway process exited before sending its port number"
     ]
    }
   ],
   "source": [
    "sc = SparkContext(conf=conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dutch-message",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "extensive-genetics",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for jovyan: \n"
     ]
    }
   ],
   "source": [
    "!sudo apt-get install -yq openjdk-8-jre openjdk-8-jre-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conditional-prayer",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-edinburgh",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "advance-papua",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://superuser.com/questions/67765/sudo-with-password-in-one-command-line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "contrary-submission",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[sudo] password for jovyan: Sorry, try again.\n",
      "[sudo] password for jovyan: \n",
      "sudo: no password was provided\n",
      "sudo: 1 incorrect password attempt\n"
     ]
    }
   ],
   "source": [
    "!echo \"jupyter\" | sudo -S apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-optimization",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "orange-language",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "unlike-species",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "biological-pierre",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

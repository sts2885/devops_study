
#커스텀 도커파일

#도대체가 이 책에서 나온 코드는 제대로 된게 하나도 없냐?

# See https://www.kubeflow.org/docs/components/notebooks/container-images/
#ARG base

#ECR public 에서 컨테이너 받으려면 awscli 통해서 ecr public login해야 함
#kubeflow-on-aws/notebook-servers/jupyter-tensorflow:2.12.0-cpu-py310-ubuntu20.04-ec2-v1.0
#public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow:tag
#FROM kubeflow-on-aws/notebook-servers/jupyter-tensorflow:2.12.0-cpu-py310-ubuntu20.04-ec2-v1.0
#tensorflow 2.5버전 주피터 서버
FROM public.ecr.aws/j1r0q0g6/notebooks/notebook-servers/jupyter-tensorflow:master-a3e67698
ARG sparkversion=3.4.1
ARG sparkrelease=spark-3.4.1-bin-without-hadoop
#ARG sparkserver https://archive.apache.org/dist/spark/spark-3.4.1/spark-3.4.1-bin-without-hadoop.tgz
ARG sparkserver=https://archive.apache.org/dist/spark

# We need to run as root for updates
USER root
# Set an environment variable for where we are going to put Spark 주
ENV SPARK_HOME /opt/spark
ENV JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64
ENV PATH=$PATH:${JAVA_HOME}/bin:${SPARK_HOME}/bin:${SPARK_HOME}/sbin
ENV PYSPARK_PYTHON=/usr/bin/python3
 

# Install Java because Spark needs it
#RUN apt-get update && \
# apt-get install -yq openjdk-8-jre openjdk-8-jre-headless && \
# apt-get clean && \
# rm -rf /var/lib/apt/lists/*

#진짜 쓸모없는것만 바글바글하게 깔고 실제 돌리면 하나도 안돌아가네

#https://hyem207.tistory.com/44

RUN apt-get update
RUN apt-get install -y openjdk-8-jdk


# Install Spark
#RUN set -ex && \
# rm /bin/sh && \
# ln -sv /bin/bash /bin/sh
RUN echo "Setting up spark"
#RUN cd /tmp && \
# (wget ${sparkserver}/spark-${sparkversion}/${sparkrelease}.tgz) && \
# cd /opt && tar -xvf /tmp/${sparkrelease}.tgz && \
# rm /tmp/${sparkrelease}.tgz && mv ${sparkrelease} spark && \
# cd spark/python && pip install -e .

#wget https://dlcdn.apache.org/spark/spark-3.1.2/spark-3.1.2-bin-hadoop3.2.tgz
COPY spark /opt/spark

RUN cd /opt/spark/python && pip install -e .

RUN pip install findspark

#https://yahwang.github.io/posts/84
# hadoop-aws 3.2.0
#RUN wget -o /opt/spark/jars/hadoop-aws-3.2.0.jar https://repo1.maven.org/maven2/org/apache/hadoop/hadoop-aws/3.2.0/hadoop-aws-3.2.0.jar
# aws-java-sdk 1.11.375 (compile dependency with hadoop aws)
#RUN wget -o /opt/spark/jars/aws-java-sdk-bundle-1.11.375.jar https://repo1.maven.org/maven2/com/amazonaws/aws-java-sdk-bundle/1.11.375/aws-java-sdk-bundle-1.11.375.jar

#RUN curl https://gist.githubusercontent.com/danking/f8387f5681b03edc5babdf36e14140bc/raw/23d43a2cc673d80adcc8f2a1daee6ab252d6f667/install-s3-connector.sh | bash

RUN pip install kfp==1.8.19
RUN pip install pandas
RUN pip install opencv-python
RUN pip install numpy
RUN pip install minio==6.0.2


RUN apt install -y apt-transport-https ca-certificates curl gnupg-agent software-properties-common
RUN curl -fsSL https://download.docker.com/linux/ubuntu/gpg | apt-key add -
RUN add-apt-repository --yes "deb [arch=amd64] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable"
RUN apt update
RUN apt install -y docker-ce docker-ce-cli containerd.io


# Fix permissions
WORKDIR /opt/spark/work-dir
RUN chmod -R 777 /opt/spark/
# Switch the user back; using jovyan as a user is bad but the base image
# depends on it.
USER jovyan

#root 유저 상태에서 하면 jovyan에는 적용안되어 있음.

#ENV PYSPARK_SUBMIT_ARGS="--master local[2] pyspark-shell"

